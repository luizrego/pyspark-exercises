{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark SQL Session\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+-----+-----+\n|color|fruit| size|\n+-----+-----+-----+\n|  Red|Apple|Large|\n+-----+-----+-----+\n\n"
    }
   ],
   "source": [
    "# example simple JSON file\n",
    "df = spark.read.json(\"example_1.json\", multiLine=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "StructType(List(StructField(color,StringType,true),StructField(fruit,StringType,true),StructField(size,StringType,true)))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+\n|color|\n+-----+\n|  Red|\n+-----+\n\n"
    }
   ],
   "source": [
    "df.select(\"color\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- color: string (nullable = true)\n |-- fruit: string (nullable = true)\n |-- size: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "== Physical Plan ==\n*(1) FileScan json [color#0,fruit#1,size#2] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/lrego/Projects/pyspark_exercises/example_1.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<color:string,fruit:string,size:string>\n"
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested JSON structure"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data analysis in a nested JSON structure.\n",
    "\n",
    "https://docs.azuredatabricks.net/_static/notebooks/transform-complex-data-types-python.html  \n",
    "https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/  \n",
    "https://stackoverflow.com/questions/37471346/automatically-and-elegantly-flatten-dataframe-in-spark-sql\n",
    "https://adatis.co.uk/parsing-nested-json-lists-in-databricks-using-python/  \n",
    "https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html  \n",
    "https://stackoverflow.com/questions/34271398/flatten-nested-spark-dataframe\n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "This is a unformatted json example."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"example_3.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- persons: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- age: long (nullable = true)\n |    |    |-- cars: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- models: array (nullable = true)\n |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "== Physical Plan ==\n*(1) FileScan json [persons#25] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/lrego/Projects/pyspark_exercises/example_3.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<persons:array<struct<age:bigint,cars:array<struct<models:array<string>,name:string>>,name:...\n"
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark functions\n",
    "from pyspark.sql.functions import explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = df2.select(explode(\"persons\").alias(\"persons\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_cars = persons.select(\n",
    "   col(\"persons.name\").alias(\"persons_name\")\n",
    " , col(\"persons.age\").alias(\"persons_age\")\n",
    " , explode(\"persons.cars\").alias(\"persons_cars_brands\")\n",
    " , col(\"persons_cars_brands.name\").alias(\"persons_cars_brand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_cars_models = persons_cars.select(\n",
    "   col(\"persons_name\")\n",
    " , col(\"persons_age\")\n",
    " , col(\"persons_cars_brand\")\n",
    " , explode(\"persons_cars_brands.models\").alias(\"persons_cars_model\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------+-----------+------------------+------------------+\n|persons_name|persons_age|persons_cars_brand|persons_cars_model|\n+------------+-----------+------------------+------------------+\n|        John|         30|              Ford|            Fiesta|\n|        John|         30|              Ford|             Focus|\n|        John|         30|              Ford|           Mustang|\n|        John|         30|               BMW|               320|\n|        John|         30|               BMW|                X3|\n|        John|         30|               BMW|                X5|\n|       Peter|         46|           Huyndai|               i10|\n|       Peter|         46|           Huyndai|               i30|\n|       Peter|         46|          Mercedes|              E320|\n|       Peter|         46|          Mercedes|           E63 AMG|\n+------------+-----------+------------------+------------------+\n\n"
    }
   ],
   "source": [
    "persons_cars_models.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}